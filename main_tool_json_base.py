import requests
import json
import subprocess
import platform
import time
import os
import sys
import argparse
import re

OLLAMA_URL = "http://localhost:11434"
OLLAMA_MODEL = "qwen3:4b"#"gemma3:12b-it-qat" #"qwen2.5:3b"
MCP_URL = "http://localhost:5005/execute"
TOOLS_URL = "http://localhost:5005/tools"

def clean_llm_json_response(response_text: str):
    """æ¸…ç† LLM å›å‚³çš„ JSON æ­¥é©Ÿé™£åˆ—ï¼Œä½¿å…¶å¯è¢« json.loads å®‰å…¨è§£æ"""

    # === Step 1: ç§»é™¤ f-string èªæ³• ===
    response_text = response_text.replace('f"', '"').replace("f'", "'")

    # === Step 2: ä¿®è£œè®Šæ•¸èªæ³•ä¸å®Œæ•´ ===
    response_text = response_text.replace('"${', '"${{').replace('}"', '}}')

    # === Step 3: ç§»é™¤ markdown code block ===
    response_text = re.sub(r"```(?:json)?\s*\n(.*?)```", r"\1", response_text, flags=re.DOTALL)

    # === Step 4: å˜—è©¦æŠ“å‡º JSON array å€å¡Šï¼ˆé¿å…å¤šé¤˜æ•˜è¿°ï¼‰===
    array_match = re.search(r"\[\s*\{.*?\}\s*\]", response_text, re.DOTALL)
    if array_match:
        response_text = array_match.group(0)

    # === Step 5: æ¸…é™¤è¨»è§£ //ã€å¤šé¤˜é€—è™Ÿ ===
    response_text = re.sub(r"//.*?$", "", response_text, flags=re.MULTILINE)
    response_text = re.sub(r",\s*}", "}", response_text)
    response_text = re.sub(r",\s*]", "]", response_text)

    # === Step 6: ä¿®è£œè®Šæ•¸æ’å…¥èªæ³•éŒ¯èª¤ ===
    # ${xxx} â†’ ${{xxx}}
    response_text = re.sub(r'\$\{([a-zA-Z0-9_]+)\}', r'${{\1}}', response_text)

    # ${{xxx}_abc â†’ ${{xxx}}_abc
    response_text = re.sub(r'\$\{\{([a-zA-Z0-9_]+)\}_', r'${{\1}}_', response_text)

    # ${{xxx}/abc â†’ ${{xxx}}/abc
    response_text = re.sub(r'\$\{\{([a-zA-Z0-9_]+)\}/', r'${{\1}}/', response_text)

    # ${{xxx} â†’ ${{xxx}}
    response_text = re.sub(r'\$\{\{([a-zA-Z0-9_]+)\}(?!\})', r'${{\1}}', response_text)

    # "${{xxx}, â†’ "${{xxx}}",
    response_text = re.sub(r'"\$\{\{([a-zA-Z0-9_]+)\},', r'"${{\1}}",', response_text)

    # ä¿®è£œåƒ "${{xxx}, â†’ "${{xxx}}",
    response_text = re.sub(r'"(\$\{\{[a-zA-Z0-9_]+)\},', r'"\1}}",', response_text)

    # å¦‚æœ "${{xxx}}" æ²’æœ‰æ”¶å°¾é›™å¼•è™Ÿï¼Œå¹«ä»–è£œä¸Šï¼ˆä¾‹å¦‚å‡ºç¾åœ¨ value çµå°¾ï¼‰
    response_text = re.sub(r'("\$\{\{[a-zA-Z0-9_]+\}\})(?=\s*[},])', r'\1"', response_text)

    # ç§»é™¤ <think> ... </think> å€æ®µ
    response_text = re.sub(r"<think>.*?</think>", "", response_text, flags=re.DOTALL)

    return response_text

def resolve_param_value(value: str, context: dict) -> str:
    """
    æ ¹æ“š context è§£æè®Šæ•¸æ’å€¼
    æ”¯æ´æ ¼å¼ï¼š
    - "${var}" æˆ– "${{var}}"
    - è¤‡åˆå­—ä¸²ï¼Œå¦‚ "${{a}}/${{b}}/x.txt"
    """

    if not isinstance(value, str):
        return value

    # æ”¯æ´è¤‡åˆå­—ä¸²æ’å€¼ï¼Œå¦‚ "${{a}}/subdir/${{b}}"
    def replacer(match):
        var_name = match.group(1)
        return str(context.get(var_name, f"${{{{{var_name}}}}}"))  # è‹¥æ‰¾ä¸åˆ°ä¿ç•™åŸæ¨£

    # åŒ¹é… ${{var}} æˆ– ${var}
    return re.sub(r"\$\{\{?([a-zA-Z0-9_]+)\}?\}", replacer, value)

# === å•Ÿå‹• Ollama Serverï¼ˆå¦‚æœå°šæœªåŸ·è¡Œï¼‰===
def is_ollama_running():
    try:
        res = requests.get(f"{OLLAMA_URL}/api/version", timeout=2)
        return res.status_code == 200
    except:
        return False

def start_ollama_server():
    print("ğŸŸ¡ Ollama æœªå•Ÿå‹•ï¼Œæ­£åœ¨å•Ÿå‹•ä¸­...")
    if platform.system() == "Windows":
        subprocess.Popen(["ollama", "serve"], creationflags=subprocess.CREATE_NEW_CONSOLE)
    else:
        subprocess.Popen(["ollama", "serve"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    for i in range(10):
        if is_ollama_running():
            print("âœ… Ollama å·²å•Ÿå‹•")
            return True
        time.sleep(1)
    print("âŒ ç„¡æ³•å•Ÿå‹• Ollamaï¼Œè«‹ç¢ºèªå®‰è£èˆ‡è¨­å®šã€‚")
    return False

# === å·¥å…·æŸ¥è©¢ ===
def get_available_tools():
    res = requests.get(TOOLS_URL)
    return res.json()

# === å‘ LLM ç”¢ç”Ÿæ­¥é©Ÿ ===
def generate_plan(task, tools):
    tool_descriptions = "\n".join([
        f"- {tool['name']}({', '.join(tool['parameters'].keys())})"
        for tool in tools
    ])

    prompt = f"""
You are a task planner.
Your job is to break down the following task into a precise sequence of tool calls.
Each tool call may generate output, which must be used in subsequent steps.

ğŸ› ï¸ Available tools:
{tool_descriptions}

ğŸ§© Output format:
- A JSON array of steps.
- Each step must be a JSON object with:
  - `"action"`: tool name
  - `"params"`: input dictionary (omit if none)
- DO NOT use Python f-strings or `${{var}}_suffix` formatting.
- DO use full variable substitution only: `"path": "${{get_desktop_path_result}}"`

âš ï¸ Requirements:
- If a step needs a value from another tool, you must call that tool first.
- You can only reference outputs using this format: `"${{toolname_result}}"`
- Combine strings by splitting them into separate parameters (e.g., `folder_name` and later `filename`, not combined).
- Avoid hardcoded values when a tool can provide the value.

ğŸ¯ Task:
{task}

âœ… Example output:
[
  {{"action": "get_desktop_path"}},
  {{"action": "get_current_time"}},
  {{"action": "create_folder", "params": {{
    "path": "${{get_desktop_path_result}}",
    "folder_name": "${{get_current_time_result}}"
  }}}},
  {{"action": "write_text_file", "params": {{
    "path": "${{get_desktop_path_result}}/${{get_current_time_result}}/log.txt",
    "content": "é€™æ˜¯è‡ªå‹•å»ºç«‹çš„æ—¥èªŒ"
  }}}}
]

ğŸ›‘ DO NOT explain anything. Just return the JSON array of steps.

"""


    response = requests.post(f"{OLLAMA_URL}/api/generate", json={
        "model": OLLAMA_MODEL,
        "prompt": prompt,
        "stream": False
    })
    response.raise_for_status()
    result = response.json()
    
    # Clean and fix the response to make it valid JSON
    response_text = result["response"]

    response_text = clean_llm_json_response(response_text)

    try:
        return json.loads(response_text)
    except json.JSONDecodeError:
        print("âŒ ç„¡æ³•è§£æ LLM å›æ‡‰ç‚ºæœ‰æ•ˆ JSONï¼š")
        print(response_text)
        return []


# === åŸ·è¡Œæ­¥é©Ÿ ===
def execute_steps(steps):
    context = {}
    execution_log = []

    for idx, step in enumerate(steps, 1):
        action = step["action"]
        raw_params = step.get("params", {})

        resolved_params = {}
        for k, v in raw_params.items():
            resolved_params[k] = resolve_param_value(v, context)

        print(f"\nğŸ› ï¸ Step {idx}: {action}({resolved_params})")
        res = requests.post(MCP_URL, json={"action": action, "params": resolved_params})
        if res.status_code != 200:
            result = f"âŒ Error: {res.json().get('detail')}"
        else:
            result = res.json().get("result")

        context[f"{action}_result"] = result
        execution_log.append({
            "step": idx,
            "action": action,
            "params": resolved_params,
            "result": result
        })

    return execution_log

# === ä¸»ç¨‹å¼ ===
def main():

    task = "è«‹åœ¨æ¡Œé¢å»ºç«‹ä¸€å€‹åç‚º test123 çš„è³‡æ–™å¤¾"
    #task = "è«‹å¹«æˆ‘è®€å–æ¡Œé¢çš„ notes.txt ä¸¦å¹«æˆ‘ç¿»è­¯æˆè‹±æ–‡ï¼Œå„²å­˜ç‚º translated.txt"
    #task = "è«‹æŠŠæ¡Œé¢ test123 è³‡æ–™å¤¾å£“ç¸®ç‚º zip å­˜åœ¨ Downloads ç›®éŒ„ä¸­"
    #task = "è«‹åœ¨æ¡Œé¢å»ºç«‹ä¸€å€‹ä»¥ä»Šå¤©æ—¥æœŸå‘½åçš„è³‡æ–™å¤¾ï¼Œä¸¦å»ºç«‹ä¸€å€‹ log.txt æª”æ¡ˆï¼Œå¯«å…¥ã€é€™æ˜¯è‡ªå‹•å»ºç«‹çš„æ—¥èªŒã€"
    #task = "è«‹å¹«æˆ‘è®€å–æ¡Œé¢çš„ notes.txt ä¸¦å¹«æˆ‘ç¿»è­¯æˆè‹±æ–‡ï¼Œå„²å­˜ç‚º translated.txt"
    #task = "è«‹æŠŠæ¡Œé¢ test123 è³‡æ–™å¤¾å£“ç¸®ç‚º zip å­˜åœ¨ Downloads ç›®éŒ„ä¸­"
    #task = "è«‹åœ¨æ¡Œé¢å»ºç«‹ä¸€å€‹ä»¥ä»Šå¤©æ—¥æœŸå‘½åçš„è³‡æ–™å¤¾ï¼Œä¸¦å»ºç«‹ä¸€å€‹ log.txt æª”æ¡ˆï¼Œå¯«å…¥ã€é€™æ˜¯è‡ªå‹•å»ºç«‹çš„æ—¥èªŒã€"

    if not is_ollama_running():
        if not start_ollama_server():
            exit(1)

    print("ğŸ” å–å¾—å·¥å…·æ¸…å–®ä¸­...")
    tools = get_available_tools()

    print("ğŸ§  å‘ LLM è¦æ±‚æ­¥é©Ÿè¨ˆç•«...")
    steps = generate_plan(task, tools)
    print(json.dumps(steps, indent=2, ensure_ascii=False))

    print("ğŸš€ åŸ·è¡Œæ­¥é©Ÿä¸­...")
    logs = execute_steps(steps)

    print("\nğŸ“œ å®Œæ•´åŸ·è¡Œçµæœï¼š")
    print(json.dumps(logs, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    main()

import json
import requests
import time

OLLAMA_URL = "http://localhost:11434/api/chat"

def stream_chat(model, messages):
    response = requests.post(
        OLLAMA_URL,
        headers={"Content-Type": "application/json"},
        data=json.dumps({
            "model": model,
            "messages": messages,
            "stream": True
        }),
        stream=True
    )

    full_reply = ""
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "message" in chunk and "content" in chunk["message"]:
                content = chunk["message"]["content"]
                print(content, end="", flush=True)
                full_reply += content
            if chunk.get("done", False):
                break
    print()
    return full_reply.strip()

def run_dual_llm_debate(user_question):
    print("ğŸ§  å•é¡Œï¼š", user_question)
    print("ğŸ” å•Ÿå‹• Assistant1 / Assistant2 å°è©±...\n")

    # åˆå§‹è§’è‰²è¨­å®š
    system_prompt_1 = "You are Assistant1. Your job is to solve the user's question thoroughly and logically."
    system_prompt_2 = "You are Assistant2. Your job is to question and challenge the reasoning or assumptions made by Assistant1."

    # å°è©±æ­·å²ç´€éŒ„
    conversation = [
        {"role": "user", "content": user_question}
    ]

    assistant1_model = "qwen2.5:3b"
    assistant2_model = "qwen3:4b"

    # åŠ å…¥ system prompt
    conversation_1 = [{"role": "system", "content": system_prompt_1}] + conversation.copy()
    conversation_2 = [{"role": "system", "content": system_prompt_2}] + conversation.copy()

    for round in range(3):
        print(f"\nğŸ”· Round {round+1} - Assistant1 å›ç­”")
        assistant1_reply = stream_chat(assistant1_model, conversation_1)
        conversation_1.append({"role": "assistant", "content": assistant1_reply})
        conversation_2.append({"role": "assistant", "content": assistant1_reply})

        print(f"\nğŸ”¶ Round {round+1} - Assistant2 è³ªç–‘")
        assistant2_reply = stream_chat(assistant2_model, conversation_2)
        conversation_1.append({"role": "user", "content": assistant2_reply})
        conversation_2.append({"role": "user", "content": assistant2_reply})

    # æœ€å¾Œè¦æ±‚ Assistant1 çµ¦å‡ºçµè«–
    final_question = "Please give us the final conclusion."
    print("\nğŸ”š Assistant2: (çµå°¾è³ªç–‘) â†’", final_question)
    conversation_1.append({"role": "user", "content": final_question})
    print("\nğŸ¯ Assistant1 æœ€çµ‚çµè«–ï¼š")
    final_reply = stream_chat(assistant1_model, conversation_1)

    print("\nâœ… å°è©±çµæŸã€‚")

if __name__ == "__main__":
    user_question = input("è«‹è¼¸å…¥å•é¡Œï¼š ").strip()
    run_dual_llm_debate(user_question)

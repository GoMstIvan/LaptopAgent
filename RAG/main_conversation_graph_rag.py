import os
import json
import numpy as np
import requests
import time
import platform
import subprocess
import sys
from typing import List, Dict
from sklearn.metrics.pairwise import cosine_similarity

from conversation_handler import ConversationHandler

# === è¨­å®š ===
OLLAMA_URL = "http://localhost:11434"
OLLAMA_CHAT_URL = f"{OLLAMA_URL}/api/chat"
OLLAMA_EMBED_URL = f"{OLLAMA_URL}/api/embeddings"
EMBED_MODEL = "shaw/dmeta-embedding-zh"
GRAPH_PATH = "rag/graph.json"


# === è¼‰å…¥åœ–è³‡æ–™èˆ‡åŸ·è¡Œ GraphRAG æŸ¥è©¢ ===
def load_graph_data() -> Dict:
    with open(GRAPH_PATH, "r", encoding="utf-8") as f:
        return json.load(f)


def get_query_embedding(query: str) -> List[float]:
    response = requests.post(OLLAMA_EMBED_URL, json={
        "model": EMBED_MODEL,
        "prompt": query
    })
    response.raise_for_status()
    return response.json()["embedding"]


def graph_rag_retrieve(query: str, top_k=5) -> List[Dict]:
    graph = load_graph_data()
    nodes = graph["nodes"]
    embeddings = []

    for node in nodes:
        desc = node["description"]
        response = requests.post(OLLAMA_EMBED_URL, json={
            "model": EMBED_MODEL,
            "prompt": desc
        })
        response.raise_for_status()
        node_embedding = response.json()["embedding"]
        embeddings.append({
            "node": node,
            "embedding": node_embedding
        })

    query_embedding = np.array(get_query_embedding(query))
    matrix = np.array([np.array(e["embedding"]) for e in embeddings])
    similarities = cosine_similarity([query_embedding], matrix)[0]

    ranked = sorted(zip(embeddings, similarities), key=lambda x: x[1], reverse=True)
    return [dict(node=item[0]["node"], score=item[1]) for item in ranked[:top_k]]


# === å°è©±æ§åˆ¶å™¨ ===
class ConversationController:
    def __init__(self, model="qwen2.5:3b"):
        self.model = model
        self.conversation_handler = ConversationHandler()
        self.conversation_id = self.conversation_handler.new_conversation(model)
        print(f"ğŸ†• Started new conversation with ID: {self.conversation_id}")

    def chat_with_llm(self, user_input: str):
        self.conversation_handler.add_message("user", user_input)

        # ğŸ” GraphRAG æª¢ç´¢æœ€ç›¸é—œçš„å…¬å¸ç¯€é»æè¿°
        context_entries = graph_rag_retrieve(user_input, top_k=5)
        context_text = "\n\n".join([
            f"[{item['node']['id']}]: {item['node']['description']}"
            for item in context_entries
        ])

        # æº–å‚™ messages çµæ§‹ï¼ˆåŠ å…¥ç³»çµ±æç¤º + æª¢ç´¢è£œå……ï¼‰
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°ç§‘æŠ€ç”¢æ¥­å…¬å¸ç†Ÿæ‚‰çš„åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šä»¥ä¸‹å…¬å¸è³‡è¨Šèˆ‡å°è©±æ­·å²å›è¦†å•é¡Œã€‚\n\n" + context_text}
        ]
        messages += [
            {"role": msg["role"], "content": msg["content"]}
            for msg in self.conversation_handler.get_conversation_history()
        ]

        try:
            response = requests.post(
                OLLAMA_CHAT_URL,
                headers={"Content-Type": "application/json"},
                data=json.dumps({
                    "model": self.model,
                    "messages": messages,
                    "stream": True
                }),
                stream=True
            )

            if response.status_code != 200:
                print(f"\nâŒ Failed to get response: {response.status_code}")
                print(response.text)
                return

            print("\nAssistant: ", end="", flush=True)
            full_response = ""

            for line in response.iter_lines():
                if line:
                    try:
                        chunk = json.loads(line)
                        if "message" in chunk and "content" in chunk["message"]:
                            content = chunk["message"]["content"]
                            if content:
                                print(content, end="", flush=True)
                                full_response += content
                        if chunk.get("done", False):
                            break
                    except json.JSONDecodeError:
                        continue

            print()
            self.conversation_handler.add_message("assistant", full_response)

        except Exception as e:
            print(f"\nâŒ Error communicating with Ollama: {e}")


# === ä¸»ç¨‹å¼ ===
if __name__ == "__main__":
    ctrl = ConversationController(model="qwen2.5:3b")

    print("\nâœ… GraphRAG æ¨¡å¼å•Ÿå‹•ï¼šç›´æ¥è¼¸å…¥å•é¡Œé–‹å§‹å°è©±")

    while True:
        try:
            user_input = input("\nYou: ").strip()
            if user_input:
                ctrl.chat_with_llm(user_input)
        except KeyboardInterrupt:
            print("\nğŸ›‘ KeyboardInterrupt detected")
            ctrl.conversation_handler.save_conversation()
            break
